<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Symbolic Execution Tech</title>
      <link href="/2022/10/27/Symbolic-Execution-Tech/"/>
      <url>/2022/10/27/Symbolic-Execution-Tech/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Survey-of-Symbolic-Execution-Tech"><a href="#A-Survey-of-Symbolic-Execution-Tech" class="headerlink" title="A Survey of Symbolic Execution Tech"></a>A Survey of Symbolic Execution Tech</h1><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><p>Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and Irene Finocchi<br>Sapienza University of Rome</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The key idea is to allow a program to take on <em>symbolic</em>-rather than <em>concrete</em>-input values.<br>Execution is performed by a symbolic execution engine.<br>It maintains for each explored control flow path:</p><p>(1) a first-order Boolean <em>formula</em> that describes the conditions satisfied by the branches taken along that path,</p><p>(2) a <em>symbolic memory store</em> that maps variables to symbolic expressions or values.</p><p>Evert value that cannot be determined by a static analysis of the code, such as an <strong>actual parameter</strong> of a function or the <strong>result of a system call</strong> that reads data from a stream, is <strong>represented by a symbol $\alpha_i$</strong>.</p><h4 id="At-any-time-the-symbolic-execution-engine-maintains-a-state-stmt-sigma-pi-where"><a href="#At-any-time-the-symbolic-execution-engine-maintains-a-state-stmt-sigma-pi-where" class="headerlink" title="At any time, the symbolic execution engine maintains a state $(stmt, \sigma, \pi)$ where:"></a>At any time, the symbolic execution engine maintains a state $(stmt, \sigma, \pi)$ where:</h4><ul><li><p><em>stmt</em> is the next statement to evaluate. It can be an assignment, a conditional branch, or a jump.</p></li><li><p>$\sigma$ is a symbolic store that associates program variables with either expressions over concreate values or symbolic values $ \alpha_i $.</p></li><li><p>$\pi$ denotes the <strong>path constraints</strong>, i.e., is a formula that expresses a set of assumptions on the symbols $\alpha_i$ due to branches taken in the execution execution to reach <em>stmt</em>. At the beginning of the analysis, $\pi = true$.</p></li></ul><h4 id="Depending-on-stmt-the-symbolic-engine-changes-the-state-as-follows"><a href="#Depending-on-stmt-the-symbolic-engine-changes-the-state-as-follows" class="headerlink" title="Depending on stmt, the symbolic engine changes the state as follows:"></a>Depending on <em>stmt</em>, the symbolic engine changes the state as follows:</h4><ul><li><p>The evaluation of an assignment $x=e$ updates the symbolic store $\sigma$ by associating x with a new symbolic expression $e_s$.</p></li><li><p>The evaluation of a conditional branch if $e$ then $s_true$ else $s_false$ affects the path constraints $\pi$.</p></li><li><p>The evaluation of a jump <em>goto s</em> updates the execution state by advancing the symbolic execution to statement <em>s</em>.</p></li></ul><h4 id="Challenges-in-Symbolic-Execution"><a href="#Challenges-in-Symbolic-Execution" class="headerlink" title="Challenges in Symbolic Execution"></a>Challenges in Symbolic Execution</h4><ul><li><p>Memory: pointers, arrays and other complex objects?</p></li><li><p>Enviroment: interactions across software stacks?</p></li><li><p>State space explosion: path explosion?</p></li><li><p>Constraint solving: in practice?</p></li></ul><h2 id="Symbolic-Execution-Engines"><a href="#Symbolic-Execution-Engines" class="headerlink" title="Symbolic Execution Engines"></a>Symbolic Execution Engines</h2><p>important principles for the design of symbolic executors</p><h4 id="Mixing-Symbolic-and-Concrete-Execution-Concolic"><a href="#Mixing-Symbolic-and-Concrete-Execution-Concolic" class="headerlink" title="Mixing Symbolic and Concrete Execution (Concolic)"></a>Mixing Symbolic and Concrete Execution (Concolic)</h4><ul><li><p><strong>Main limitation of classical symbolic execution.</strong> It cannot explore feasible executions that would  result in path constraints that cannot be dealt with. For example, external code not traceable by the executor, complex constraints involving (e.g., non-linear arithmetic or transcendental functions)</p></li><li><p><strong>Dynamic Symbolic Execution.</strong> the execution engine maintains a concrete store $\sigma_c$. As a consequence, the symbolic engine does not need to invoke the constraint solver to decide whether a branch condition is (un)satisfiable: this is directly tested by the concrete execution.</p></li><li><p><strong>Selective Symbolic Execution.</strong> one might want to explore only some components of a software stack in full, not caring about others.</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Decompiling x86 Deep Neural Network Executables</title>
      <link href="/2022/10/25/Decompiling-x86-Deep-Neural-Network-Executables/"/>
      <url>/2022/10/25/Decompiling-x86-Deep-Neural-Network-Executables/</url>
      
        <content type="html"><![CDATA[<h1 id="Decompiling-x86-Deep-Neural-Network-Executables"><a href="#Decompiling-x86-Deep-Neural-Network-Executables" class="headerlink" title="Decompiling x86 Deep Neural Network Executables"></a>Decompiling x86 Deep Neural Network Executables</h1><p>Zhibo Liu, The Hong Kong University of Science and Technology</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>They present BTD (Bin to DNN), a decompiler for DNN executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. <strong>Supports different DL compilers and with full optimizations enabled on x86 platforms.</strong></p><p><strong>Employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators.</strong></p><p>BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recivered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable.</p><p>They also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A DL compiler takes a high-level model specification (e.g., in ONNX format) and generates corresponding low-level optimized binary code for a variety of hardware backends (from cloud servers to embedded devices, GPUs, CPUs, and FPGAs).</p><p>Compilation of high-level models into binary code typically involves multiple optimization cycles. DL compiler can optimize code utilizing domain-specific hardware features and abstractions. However, they observe that different low-level representaions of the same DNN operator in executables generally retain invariant high-level semantics, as DNN operators like ReLU and Sigmoid, are mathematically defined in a rigorous manner.</p><p>They proposed a three-step approach for full recovery of DNN operators, network topology, dimensions, and parameters.</p><ul><li>BTD conducts representation learning over disassembler-emitted assembly code to classify assembly functions as DNN operators, such as convolution layers (Conv).</li><li>Dynamic analysis is then used to chain DNN operators together, thus recovering their topological connectivity.</li><li><p>To further recover dimensions and parameters of certain DNN operators (e.g., Conv), they launch trace-based symbolic execution to generate symbolic constraints, primarily over floating-point-related computations. The human-readable symbolic constraints denote semantics of corresponding DNN operators that are invariant across different compilation settings. To deliver an automated pipeline, they then define patterns over symbolic constraints to automatically recover dimensions and memory layouts of parameters. They incorporate taint analysis to largely reduce the cost of symbolic execution which is more heavy weight.</p></li><li><p>BTD is scalable to recover DNN models from 65 DNN executables, including nearly 3 million instructions, in 60 hours with negligible errors.</p></li><li><p>Moreover, to demonstrate BTD’s correctness, they rebuild decompiled model specifications with PyTroch. The results show that almost all decompiled DNN models can be recompiled into new executables that behave identically to the reference executables. </p></li></ul><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p><strong>DNN Compiler Frontend: Graph IRs and Optimizations.</strong> Convert DNN computation graphs into graph IRs. Graph IRs specify high-level inputs and outputs of each operator, but do not restrict how each operator is implemented.<br>Transformation and optimization of computation grpahs (IRs).</p><p><strong>DNN Compiler Backend: Low-Level IRs and Optimizations.</strong> Graph IR operators can be converted into low-level linear algebra operators. For example, a fully connected (FC) operator can be representated as matrix multiplication followed be addition. Low-level IRs are usually memory related. Hence, optimizations at this step can include hardware intrinsic (固有的) mapping, memory allocation, loop-related optimizations, and parallelization.<br>Transformation and optimization of low-level linear algebra operators.</p><p><strong>DNN Compiler Backend: Scheduling and Tuning.</strong> Policies mapping an operator to low-level code are called <em>schedules</em>. </p><p><strong>DNN Compiler Backend: Code Gen.</strong> When generating machine code, a DNN operator (or several fused operators) is typically compiled into an individual assembly function. </p><h2 id="Decompiling-DNN-Executables"><a href="#Decompiling-DNN-Executables" class="headerlink" title="Decompiling DNN Executables"></a>Decompiling DNN Executables</h2><p><strong>Definition.</strong> The full specifications include: (1) DNN operators(e.g., ReLU, Pooling, and Conv) and their topological connectivity, (2) dimensions of each DNN operator, such as #channels in Conv, and (3) parameters of each DNN operator, such as weights and biases, which are important configurations learned during model training.</p><p><strong>Comparison with C/C++ Decompilation.</strong></p><ul><li>Statements vs. Higher-Level Semantics: Software decompilation line-by-line translates machine instructions into C/C++ statements. </li><li>Common Uncertainty: There is no fixed mapping between C/C++ statements and assembly instructions. DL compilers may adopt different optimizations for compiling the same DNN operators. The compiled code may exhibit distinct syntactic forms. Nevertheless, the semantics of DNN operators are retained.</li><li>End Goal: Software decompilation is fundamentally undecidable, and decompiled C/C++ code mainly aids (human-based) analysis and comprehension, not recompilation. Besides helping (human-based) comprehension, BTD boosts model reuse, migration, security hardening, and adversarial attacks.</li></ul><p>NN的反编译主要是一个分类问题，C/C++的反编译是生成问题。 </p><p><strong>Opacity in DNN Executables.</strong> </p><p>Different compilers and optimizations can result in complex and distinct machine code realizations. </p><p><strong>Design Focus.</strong> BTD is designed to process common DNN models compiled by standard DL compilers. </p><h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>Deduce high-level model specifications from low-level instructions. </p><p>We advocate DL decompilers to satisfy the following criteria:</p><ul><li><strong>R1 (Generalizability):</strong> Avoid brittle assumptions. Generalize across compilers, optimizations, and versions.</li><li><strong>R2 (Correctness):</strong> Use effective, resilient methods and produce correct outputs.</li><li><strong>R3 (Performance):</strong> Be efficient when necessary.</li><li><strong>R4 (Automation):</strong> Avoid manual analysis and automate the decompilation process.</li></ul><h4 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h4><p>(1) Learning-based techniques for recognizing assembly function as DNN operators like Conv.</p><p>(2) Reconstruct the network topology using dynamic analysis.</p><p>(3) Use trace-based symbolic execution to extract operator semantics from assembly code and then recover dimensions and parameters with semantics-based patterns. Some operators are too costly for symbolic execution to analyze. They use taint analysis to keep only tainted sub-traces for more expensive symbolic execution to analyze.</p><p>(4) BTD produces model specifications that behave identically to original models. BTD does not guarantee 100% correct outputs. Procedures users can follow to fix errors.</p><ul><li><p><strong>Type I</strong> operators, including activation functions like ReLU and element-wise arithmetic operators, do not ship with parameters; recovering their dimensions is trivial.</p></li><li><p><strong>Type II and III</strong> operators require dimensions or parameters, such as Polling’s stride <em>S</em> and kernel size <em>K</em>.</p></li><li><p><strong>Type IV</strong> operators require both parameters and dimensions.</p></li></ul><p><strong>Compilation Provenance.</strong> (1) which DL compiler is used, and (2) whether e is compiled with full optimization -O3 or no optimization -O0. The authors extend their learning-based method to predict compilation provenance from assembly code.</p><p>Some patterns are designed separately for </p>]]></content>
      
      
      <categories>
          
          <category> 编译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decompiler </tag>
            
            <tag> NN Decompiler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeurDP</title>
      <link href="/2022/10/21/NeurDP/"/>
      <url>/2022/10/21/NeurDP/</url>
      
        <content type="html"><![CDATA[<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>近年来，神经网络在代码理解和逆向工程领域取得了广泛的应用。一些工作，例如著名的Github Copilot和CodeBERT等使用神经网络来学习源码的表示，在代码生成、补全和注释生成等方面取得了较好的效果。同样的，许多工作研究神经网络在二进制或汇编代码理解中的利用，从而帮助分析人员进行反汇编、恶意代码检测等。神经网络在程序分析和逆向工程中的应用大幅度帮助分析人员节省了时间成本。并且，和传统的方法相比，其速度通常较快，在有GPU的条件下可以很好的和下游任务模型进行结合，实现大批量快速的逆向工程和程序分析。</p><p>反编译器是逆向工程和程序分析中常用的工具，对于漏洞发现和恶意代码分析有重要作用。反编译过程可以看作是将低级程序语言翻译到和它功能等价的高级程序语言的过程。传统的反编译器的开发需要很多逆向工程师总结其程序分析经验，将其制定成规则，并结合程序分析技术来实现。著名的开源反编译器RetDec有上百个开发者曾为其贡献代码，并且开发多年，至今仍然不完善。</p><p>机器翻译模型在自然语言翻译的领域取得了重大进展。一些研究者受到自然语言翻译技术以及AI在代码领域中的发展的启发，开始探索使用端到端的机器翻译模型来将低级程序语言翻译成高级程序语言。这些方法尽管对于输入和输出进行了各种预处理，设计了各种模型结构，但是他们仍然无法解决优化后二进制的反编译问题。</p><h2 id="研究困难"><a href="#研究困难" class="headerlink" title="研究困难"></a>研究困难</h2><p>编译器优化在真实的项目中是广泛使用的。常用的GCC和Clang编译器都是默认开启O1级别的优化。编译器常常是两段式的结构，分为前端和后端，分别进行机器无关的优化和机器相关的优化。经过对编译技术的分析和实验，我们发现训练一个能处理优化后二进制代码的端到端的反编译模型非常困难。因为模型常常是数据驱动的，训练代码翻译模型需要高质量的低级语言和高级语言代码对。然而由于编译器优化策略，优化后的低级语言和高级语言在文本所包含的信息上常常难以对应，并且其结构差异很大。已有的神经网络反编译方法在解决优化问题上主要存在两个难点：</p><p>1）编译优化策略通常大幅度改变源码的结构。这种修改不仅体现在指令类型等层面，更多的是结构上的改变，有一些优化策略甚至会提前对一些语句进行运算，预测其结果，例如常量传播。这种改变会导致源码中有很多冗余的文本，其语义在二进制代码中本身就不存在。端到端的训练需要学习优化策略，例如上文提到的算术运算，训练这样的模型是很困难的。</p><p>2）对低级程序语言（二进制文件）和高级程序语言（源码文件）进行划分，并形成准确的对应关系是不容易的。以往的工作通常使用函数或者基本块进行划分，但是函数和基本块内的语句数量是没有上限的，对于较大的基本块或函数，模型很难对其进行翻译。根据调试信息划分面临着和难点1一样的问题。而如果设置最大窗口顺序划分指令序列，又会面临对应不准确的问题。</p><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>NeurDP主要解决当前基于神经网络的反编译方法无法解决优化后二进制反编译的问题。其中预处理部分对LPL进行一些静态分析，包括SSA形式转换，和一些简单的寄存器传播生成低级中间语言LIR。OTU是方法的核心部分，根据数据依赖关系将LIR和HIR划分为更小且可以准确匹配的代码片段。HIR是基于LLVM IR简化版本的高级中间语言。NeurDP模型部分使用图神经网络GNN学习LIR的数据依赖图，并生成HIR指令序列。HIR Generation对生成的指令序列操作数进行识别和填充。最终，结合函数的参数、返回值以及控制结构，并使用规则将HIR转换为HPL，从而生成高级语言函数。</p><p><img src="NeurDP-overview.jpg" alt=""></p><p>为了解决困难1，本方法使用中间表示IR作为连接LPL和HPL的桥梁。IR经过了编译器前端优化，因此其文本上包含的信息和结构与LPL更加的接近。而从IR到HPL通过制定规则可以很容易实现。因此，我们选择用模型去完成从LPL到IR的翻译。</p><p>为了解决困难2，本方法设计了一种基于数据依赖图的切分方式，将基本块切分成更小的代码片段。基于我们的分析，绝大多数的优化策略都是基于数据流分析的。优化策略通常不会改变基本块的输入和输出的对应关系。两个不同输出在数据依赖图中的子图存在重叠部分和独立部分。考虑到对一个输出的优化不能对另一个输出的结果产生影响，优化往往是在这些重叠或独立部分的内部发生的。因此，这些子图在HIR和LIR中可以较准确地进行对应。我们按照这个原则对LIR和HIR的基本块进行划分，获得更细粒度且对应准确的LIR-HIR代码片段对作为训练集。</p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>我们测试了其他网络在使用源码，AST和IR时代码翻译的表现和NeurDP进行对比，结果如下表。可以看到使用IR之后普通的序列模型准确率也有大幅度地提升。NeurDP使用图神经网络对输入进行编码，效果比其他序列模型更好。</p><p><img src="NeurDP-Table6.jpg" alt=""></p><p>我们测试了使用不同的基本块切分方式的代码翻译的表现，结果如下图，可以看到基于数据流的划分方式比顺序滑动窗口效果明显提升。而使用我们的OTU比普通的数据流的窗口效果更好。</p><p><img src="NeurDP-Table5.jpg" alt=""></p><p>为了验证NeurDP对优化后代码的处理能力，我们将他与其他两个神经网络反编译工作Neutron和Coda以及著名的开源反编译工具RetDec在clang的O0-O3优化级别下的准确率进行了对比，结果如下表。可以看到NeurDP效果比其他工具都好，并且对于不同优化级别的代码，NeurDP的表现差异较小。RetDec由于其功能还不完善，出现了很多函数无法识别的错误。</p><p><img src="NeurDP-Table4.jpg" alt=""></p><p>Strip是一个从二进制中去掉调试和符号信息的工具，会增加反编译的难度。我们对比了在保留所有调试和符号信息、去掉调试信息和去掉所有调试和符号信息的条件下，NeurDP在O0-O3三个优化级别上的表现。结果如下表，可以看到NeurDP在去掉符号信息的情况下也表现良好。</p><p>我们测试使用的数据集公开在了<a href="https://github.com/zijiancogito/neur-dp-data">https://github.com/zijiancogito/neur-dp-data</a> 。</p>]]></content>
      
      
      <categories>
          
          <category> 编译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decompiler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DnD: A Cross-Architecture Deep Neural Network Decompiler</title>
      <link href="/2022/10/21/DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler/"/>
      <url>/2022/10/21/DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler/</url>
      
        <content type="html"><![CDATA[<h1 id="DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler"><a href="#DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler" class="headerlink" title="DnD: A Cross-Architecture Deep Neural Network Decompiler"></a>DnD: A Cross-Architecture Deep Neural Network Decompiler</h1><p>Ruoyu Wu, Purdue University</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper proposes DnD, the first compiler and ISA-agnostic DNN decompiler.<br>It aims to lift the binary code compiled from a DNN on <strong>edge-device</strong> to a novel intermediate representation, able to express the high-level mathematical DNN operations.<br>They evaluate DnD on two compilers (<strong>Glow and TVM</strong>) and three ISAs (<strong>Thump, AArch64, and x86-64</strong>).<br>Dnd enables extracting the DNN models used by real-wrold micro-controllers and attacking them using white-box adversarial machine learning techniques.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><p>Traditional decompilers cannot capture the mathematical semantics of compiled DNN models.</p></li><li><p>DnD works with a compiled DNN model and recover its parameters, hyper-parameters and topology, and express the decompiled model in a high-level representation, encoded in the ONNX modeling language.</p></li><li><p>Techniques: 1. Uses <strong>symboloc execution</strong> in conjunction with a dedicated loop analysis to capture precise mathematical formulas representing how different DNN operators process the received data. 2. Uses a novel IR to express the high-level mathematical DNN operations in a compiler- and ISA-agnostic way. 3. Identifies the type and location of the DNN operators in a target binary by matching the extracted mathematical operations with template mathematical DNN operations, recovering hyper-parameters and parameters of all the identified DNN operators, as well as the overall network topology.</p></li><li><p>The recovered DNN model can be used to boost adversarial attacks against the original DNN, enabling the usage of the white-box attacks, in place of less efficient black-box ones.</p></li><li><p>Contributions: design and implement DnD (including decompiling of stripped binaries), design IR</p></li><li><p>Artifacts: <a href="https://github.com/purseclab/DnD">https://github.com/purseclab/DnD</a></p></li></ul><h2 id="Background-and-Motivation"><a href="#Background-and-Motivation" class="headerlink" title="Background and Motivation"></a>Background and Motivation</h2><ul><li><p>ONNX: the open standard for ML interoperability developed by Linux Foundation.</p></li><li><p>DNN Operators: the building blocks of DNNs. A DNN operator takes the output of previous operators as its input and computes its output based on its operator type and its parameters.(174 different DNN operators defined in ONNX).</p></li><li><p>DNN Hyper-parameters and Parameters: (1) the algorithm hyper-parameters: only used during the training phase (2) the model hyper-parameters: define the netwrok structure and how the operators function (Total number of operators and the type of each operator. The DNN topology. The attributes of each operator that define its detailed semantics.)</p></li><li><p>DNN Compilers: Glow, TVM, XLA, NNFusion. Frontend-Backend component. </p></li><li><p>Frontend: transforms a DNN model into a high-level IR and performs hardware-independent optimizations, such as operator fusion.</p></li><li><p>Backend: transforms a high-level IR to a low-level IR and performs hardware-specific optimizations (vectorization and loop-related optimizations).</p></li><li><p>Compilation Scheme: interpreter-based and ahead-of-time(AOT) compilation schemes.</p></li><li><p>Interpreter-based: generate DNN binaries at runtime.They usually produce two artifacts: a DNN configuration file describing the DNN model and a runtime library that contains all the DNN operator implementations.</p></li><li><p>AOT: Specialize the operator implementation for the specific compiled operator instance’s context.</p></li><li><p>Glow and TVM: An application feeds the input data to this inference function and obtains the predicted label as output.</p></li></ul><h2 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h2><h4 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow."></a>Workflow.</h4><p>(1) <strong>DNN Operator Location Identification.</strong> Recovers the CFG and identifies the location of inference function and DNN operators from the input (stripped) DNN binary.</p><p>(2) <strong>Operator Symmary Generation.</strong> </p><ul><li>Conducts loop analysis to identify loops’ information. </li><li>Leverages loop’s information to perform selective symbolic execution that extracts the output of a DNN operator as symbolic expressions of its input and parameters, which capture the mathematical semantic of a DNN operator.</li><li>Lift the symbolic expressions to the operator symmary of a DNN operator in their IR format includes the ASTs and other information.</li><li>Generates template ASTs through the afore-mentioned operator summary generation.</li></ul><p>(3) DNN Model Lifting<br>Lifts each operator summary to a DNN operator and convert it to a high-level DNN representation (i.e., an ONNX model).</p><ul><li>Matches the AST in each operator summary with a template AST to determine its DNN operator type.</li><li>Recovers the DNN topology by identify the data dependencies between DNN operators.</li><li>Recovers each DNN operator’s attributes and parameters leveraging the identified DNN operator type and DNN topology, and converts the fully-recovered DNN model to an ONNX model.</li></ul><p>In summary, they first recover the functions and CFGs. Then, they identify the intresting functions (inference function and NN operators). Then, they extract the symbolic expressions by SSE and transform them to their IR. Then, they <strong>match the IR with their AST template and determines the NN operator type</strong>. Then, they <strong>recovers the topology by identifying the data dependencies between NN operators</strong>. Finally, they recovers each NN operator’s <strong>attributes and parameters</strong> using the operator type and topology and convert it to ONNX language.</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h4 id="Generality"><a href="#Generality" class="headerlink" title="Generality"></a>Generality</h4><p>How many commonly-used DNN operators and models can be suported.</p><p>Support 59 (84%) DNN operators.<br>Fully support 30 (81%) DNN models out of the collected 37 DNN models.</p><h4 id="Correctness-Accross-different-DNN-compilers-ISAs-and-DNN-models"><a href="#Correctness-Accross-different-DNN-compilers-ISAs-and-DNN-models" class="headerlink" title="Correctness Accross different DNN compilers, ISAs, and DNN models"></a>Correctness Accross different DNN compilers, ISAs, and DNN models</h4><p>Compare the model architecture (operators and topology) and inference results of original DNN models and decompiled DNN models.</p><p>-Models: MNIST, MobileNets v2, ResNet v1</p><ul><li><p>ISAs: Thumb, AArch64, x86-64</p></li><li><p>Decompiler: Glow, TVM</p></li></ul><p>Evaluated 15 DNN binaries in total.</p><p>没有说明测试的二进制是否是strip的</p><h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><ul><li>Extraction Attack</li><li>Boosting Adversarial Attacks</li></ul><h2 id="Discussion-and-Limitations"><a href="#Discussion-and-Limitations" class="headerlink" title="Discussion and Limitations"></a>Discussion and Limitations</h2><ul><li><p>Compilers (XLA, NNFusion) which generate DNN binaries linked with open-souce mathematical libraries to leverage the tensor operations of these libraries. To support these additional compilers, we will need to implement a dedicated analysis to identify these tensor-specific library functions. This analysis could take advantage of function matching approaches.</p></li><li><p>Decompiling Binary on DNN Acceleartors. GPUs, FPGAs have very diverse ISAs that are usually not supported by the general-purpose disassemblers and the symbolic execution framework. <strong>NVIDIA provides closed-source disassemblers cuobjdump and nvidiaasm</strong>, which translate the CUDA binary into SASS assembly code. However, most details of SASS assembly code are kept secret.</p></li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>reverse engineering techniques targeting smart contract[1], control firmware[2] and Bluetooth firmware[3].</p><p>[1] Yi Zhou, Deepak Kumar, Surya Bakshi, Joshua Mason, Andrew Miller, and Michael Bailey. Erays: reverse engineering ethereum’s opaque smart contracts. In Proceedings of the USENIX Security Symposium (Usenix SEC), 2018.</p><p>[2] Taegyu Kim, Aolin Ding, Sriharsha Etigowni, Pengfei Sun, Jizhou Chen, Luis Garcia, Saman Zonouz, Dongyan Xu, and Dave (Jing) Tian. Reverse engineering and retroﬁtting robotic aerial vehicle control ﬁrmware using dispatch. In Proceedings of the ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2022.</p><p>[3] Jianliang Wu, Ruoyu Wu, Daniele Antonioli, Mathias Payer, Nils Ole Tippenhauer, Dongyan Xu, Dave Jing Tian, and Antonio Bianchi. LIGHTBLUE: Automatic Proﬁle-Aware debloating of bluetooth stacks. In Proceedings of the USENIX Security Symposium (Usenix SEC), 2021.</p>]]></content>
      
      
      <categories>
          
          <category> 编译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Decompiler </tag>
            
            <tag> NN Decompiler </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滕王阁序</title>
      <link href="/2022/10/20/%E6%BB%95%E7%8E%8B%E9%98%81%E5%BA%8F/"/>
      <url>/2022/10/20/%E6%BB%95%E7%8E%8B%E9%98%81%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 骈文 </category>
          
          <category> 最喜欢 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 置顶 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
