<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Decompiler | 春秋一语</title>
  <meta name="author" content="Zijian">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="春秋一语"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/journal.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
    <script src="/js/marked.js"></script>
    <script src="/js/comment.js"></script>
    <script src="/js/timeago.min.js"></script>
    <script src="/js/highlight.min.js"></script>
	<script src="/js/spin.min.js"></script>
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <nav id="main-nav" class="navbar  navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">春秋一语</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      

<!-- title -->
<div class="page-header ">
  <h1 class="archive-title-tag title ">Decompiler</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      
         <!-- display as entry -->
	     <div class="mypage">
	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-10-25 </div>
			<div class="article-title"><a href="/2022/10/25/Decompiling-x86-Deep-Neural-Network-Executables/" >Decompiling x86 Deep Neural Network Executables</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h1 id="Decompiling-x86-Deep-Neural-Network-Executables"><a href="#Decompiling-x86-Deep-Neural-Network-Executables" class="headerlink" title="Decompiling x86 Deep Neural Network Executables"></a>Decompiling x86 Deep Neural Network Executables</h1><p>Zhibo Liu, The Hong Kong University of Science and Technology</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>They present BTD (Bin to DNN), a decompiler for DNN executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. <strong>Supports different DL compilers and with full optimizations enabled on x86 platforms.</strong></p>
<p><strong>Employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators.</strong></p>
<p>BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recivered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable.</p>
<p>They also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A DL compiler takes a high-level model specification (e.g., in ONNX format) and generates corresponding low-level optimized binary code for a variety of hardware backends (from cloud servers to embedded devices, GPUs, CPUs, and FPGAs).</p>
<p>Compilation of high-level models into binary code typically involves multiple optimization cycles. DL compiler can optimize code utilizing domain-specific hardware features and abstractions. However, they observe that different low-level representaions of the same DNN operator in executables generally retain invariant high-level semantics, as DNN operators like ReLU and Sigmoid, are mathematically defined in a rigorous manner.</p>
<p>They proposed a three-step approach for full recovery of DNN operators, network topology, dimensions, and parameters.</p>
<ul>
<li>BTD conducts representation learning over disassembler-emitted assembly code to classify assembly functions as DNN operators, such as convolution layers (Conv).</li>
<li>Dynamic analysis is then used to chain DNN operators together, thus recovering their topological connectivity.</li>
<li><p>To further recover dimensions and parameters of certain DNN operators (e.g., Conv), they launch trace-based symbolic execution to generate symbolic constraints, primarily over floating-point-related computations. The human-readable symbolic constraints denote semantics of corresponding DNN operators that are invariant across different compilation settings. To deliver an automated pipeline, they then define patterns over symbolic constraints to automatically recover dimensions and memory layouts of parameters. They incorporate taint analysis to largely reduce the cost of symbolic execution which is more heavy weight.</p>
</li>
<li><p>BTD is scalable to recover DNN models from 65 DNN executables, including nearly 3 million instructions, in 60 hours with negligible errors.</p>
</li>
<li><p>Moreover, to demonstrate BTD’s correctness, they rebuild decompiled model specifications with PyTroch. The results show that almost all decompiled DNN models can be recompiled into new executables that behave identically to the reference executables. </p>
</li>
</ul>
<h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p><strong>DNN Compiler Frontend: Graph IRs and Optimizations.</strong> Convert DNN computation graphs into graph IRs. Graph IRs specify high-level inputs and outputs of each operator, but do not restrict how each operator is implemented.<br>Transformation and optimization of computation grpahs (IRs).</p>
<p><strong>DNN Compiler Backend: Low-Level IRs and Optimizations.</strong> Graph IR operators can be converted into low-level linear algebra operators. For example, a fully connected (FC) operator can be representated as matrix multiplication followed be addition. Low-level IRs are usually memory related. Hence, optimizations at this step can include hardware intrinsic (固有的) mapping, memory allocation, loop-related optimizations, and parallelization.<br>Transformation and optimization of low-level linear algebra operators.</p>
<p><strong>DNN Compiler Backend: Scheduling and Tuning.</strong> Policies mapping an operator to low-level code are called <em>schedules</em>. </p>
<p><strong>DNN Compiler Backend: Code Gen.</strong> When generating machine code, a DNN operator (or several fused operators) is typically compiled into an individual assembly function. </p>
<h2 id="Decompiling-DNN-Executables"><a href="#Decompiling-DNN-Executables" class="headerlink" title="Decompiling DNN Executables"></a>Decompiling DNN Executables</h2><p><strong>Definition.</strong> The full specifications include: (1) DNN operators(e.g., ReLU, Pooling, and Conv) and their topological connectivity, (2) dimensions of each DNN operator, such as #channels in Conv, and (3) parameters of each DNN operator, such as weights and biases, which are important configurations learned during model training.</p>
<p><strong>Comparison with C/C++ Decompilation.</strong></p>
<ul>
<li>Statements vs. Higher-Level Semantics: Software decompilation line-by-line translates machine instructions into C/C++ statements. </li>
<li>Common Uncertainty: There is no fixed mapping between C/C++ statements and assembly instructions. DL compilers may adopt different optimizations for compiling the same DNN operators. The compiled code may exhibit distinct syntactic forms. Nevertheless, the semantics of DNN operators are retained.</li>
<li>End Goal: Software decompilation is fundamentally undecidable, and decompiled C/C++ code mainly aids (human-based) analysis and comprehension, not recompilation. Besides helping (human-based) comprehension, BTD boosts model reuse, migration, security hardening, and adversarial attacks.</li>
</ul>
<p>NN的反编译主要是一个分类问题，C/C++的反编译是生成问题。 </p>
<p><strong>Opacity in DNN Executables.</strong> </p>
<p>Different compilers and optimizations can result in complex and distinct machine code realizations. </p>
<p><strong>Design Focus.</strong> BTD is designed to process common DNN models compiled by standard DL compilers. </p>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>Deduce high-level model specifications from low-level instructions. </p>
<p>We advocate DL decompilers to satisfy the following criteria:</p>
<ul>
<li><strong>R1 (Generalizability):</strong> Avoid brittle assumptions. Generalize across compilers, optimizations, and versions.</li>
<li><strong>R2 (Correctness):</strong> Use effective, resilient methods and produce correct outputs.</li>
<li><strong>R3 (Performance):</strong> Be efficient when necessary.</li>
<li><strong>R4 (Automation):</strong> Avoid manual analysis and automate the decompilation process.</li>
</ul>
<h4 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h4><p>(1) Learning-based techniques for recognizing assembly function as DNN operators like Conv.</p>
<p>(2) Reconstruct the network topology using dynamic analysis.</p>
<p>(3) Use trace-based symbolic execution to extract operator semantics from assembly code and then recover dimensions and parameters with semantics-based patterns. Some operators are too costly for symbolic execution to analyze. They use taint analysis to keep only tainted sub-traces for more expensive symbolic execution to analyze.</p>
<p>(4) BTD produces model specifications that behave identically to original models. BTD does not guarantee 100% correct outputs. Procedures users can follow to fix errors.</p>
<ul>
<li><p><strong>Type I</strong> operators, including activation functions like ReLU and element-wise arithmetic operators, do not ship with parameters; recovering their dimensions is trivial.</p>
</li>
<li><p><strong>Type II and III</strong> operators require dimensions or parameters, such as Polling’s stride <em>S</em> and kernel size <em>K</em>.</p>
</li>
<li><p><strong>Type IV</strong> operators require both parameters and dimensions.</p>
</li>
</ul>
<p><strong>Compilation Provenance.</strong> (1) which DL compiler is used, and (2) whether e is compiled with full optimization -O3 or no optimization -O0. The authors extend their learning-based method to predict compilation provenance from assembly code.</p>
<p>Some patterns are designed separately for Glow- and TVM-emitted executables.</p>
<h3 id="DNN-Operator-Recovery"><a href="#DNN-Operator-Recovery" class="headerlink" title="DNN Operator Recovery"></a>DNN Operator Recovery</h3><p>Train a neural model to map assembly functions to DNN operators. Use representation learning and treat x86 opcodes as language tokens.</p>
<p><strong>Atomic OPs.</strong> Define atomic OPs over x86 opcodes. Each opcode is thus split into atomic OPs. Use atomic OP sequences represent a DNN operator.</p>
<p><strong>Dividing Opcodes into Atomic OPs.</strong> BPE. They split each opcode into a sequence of characters and counted consecutive characters to find the most frequent ones.</p>
<p><strong>Learning over Atomic OPs.</strong> Train a neural identifier model with a sequence of atomic OPs from an assembly function as inputs. The model outputs a 1D vector with N dimensions (N is the total number of unique DNN operators), where multiple “1” in the vector implies that this assembly function represents several fused DNN operators (融合算子). </p>
<p><strong>From Operators to Compilation Provenance.</strong> 直接使用之前的函数的embeddings分类，认为这个任务很简单</p>
<h3 id="DNN-Network-Topology-Recovery"><a href="#DNN-Network-Topology-Recovery" class="headerlink" title="DNN Network Topology Recovery"></a>DNN Network Topology Recovery</h3><p>A DNN operator has a fixed number of inputs and outputs. Use Intel Pin, a dynamic instrucmentation tool, to hook every callsite. Record the memory addresses of inputs/outputs passed to callsites and connect two operators if the successor’s inputs match the predecessor’s outputs.</p>
<p>This step does not rely on any compiler-specific assumptions, however, dynamic analysis is needed. (Intel Pin 也局限了使用的平台，其他架构或许没有这种工具或者无法监控内存，动态分析开销也比较大）</p>
<p>This dynamic analysis is not limited by “coverage”. （存在疑问，依赖于DNN编译器的先验知识）</p>
<h3 id="Dimension-and-Parameter-Recovery"><a href="#Dimension-and-Parameter-Recovery" class="headerlink" title="Dimension and Parameter Recovery"></a>Dimension and Parameter Recovery</h3><p>Inputs and parameters are typically stored separately in memory, whereas neighbor input/parameter elements are stored contiguously. （存在疑问，是栈空间还是堆空间，连续可能是偶然的）</p>
<p><strong>General Workflow.</strong> summarize operator invariant semantics with symbolic execution.</p>
<p>(1) log execution traces and use taint analysis to shroten the traces.</p>
<p>(2) use symbolic execution to summarize the input-output constraint of each assembly function.</p>
<p>(3) infer dimensions using patterns defined over constraints and futher extract parameters.</p>
<h4 id="Trace-Logging-and-Taint-Analysis"><a href="#Trace-Logging-and-Taint-Analysis" class="headerlink" title="Trace Logging and Taint Analysis"></a>Trace Logging and Taint Analysis</h4><p>Use Intel Pin to log the execution trace of an operator’s assembly function.</p>
<p><strong>Pin takes several hours to log one trace.</strong></p>
<p>Hence, analyzing a subtrace containing one iteration of the outermost loop is sufficient (as long as a complete calculation of an output element is reflected in this subtrace).</p>
<p><strong>Taint Analysis.</strong> Use backward taint analysis to rule out instructions that are not involved in computing outputs.</p>
<p>Trace logging records each instructions’s execution context, including concrete memory address values. For each memory acess during taint propagation, they compute concrete addresses to taint/untaint memory cells accordingly.</p>
<h4 id="Symbolic-Execution"><a href="#Symbolic-Execution" class="headerlink" title="Symbolic Execution"></a>Symbolic Execution</h4><p>Launch SE over tainted x86 instructions.</p>
<p><strong>Identifying Memory Layouts.</strong> 建立了一个表，对应每个算子传入的参数应该是inputs还是parameters，还是outputs. Then collect and identify inputs and parameters’ memory addresses by querying the configuration. Identify memory addresses and classify them into weights and inputs. Cluster all addresses of the same parameter to scope that parameter’s memory region (i.e., the starting address and size).</p>
<h4 id="Dimension-Recovery"><a href="#Dimension-Recovery" class="headerlink" title="Dimension Recovery"></a>Dimension Recovery</h4><p>Conv operator</p>
<p><strong>Kernel Size K, Input Channel $I_C$, Zero Padding P.</strong> </p>
<p><strong>Output Channels $O_C$.</strong><br>这一部分需要了解相关函数的参数所代表的含义，根据内存大小和布局推测这几个要素。</p>
<h4 id="Recover-Parameters"><a href="#Recover-Parameters" class="headerlink" title="Recover Parameters"></a>Recover Parameters</h4><p>Idenfity their starting addresses and memory layouts.<br>识别存储参数的内存区域。<br>Use Pin to dump parameters to disk at runtime.<br>同样依赖于运行时。</p>
<p><strong>Handling Compiler Optimizations.</strong> SSE parallelism by reading 4 (or 8) floating numbers from contiguous memory into one register. These modify Conv’s standard memory layout, impeding parameter recovery. Similar to dimension inference, they use patterns to identify optimized layouts. </p>
<h3 id="Executables-Emitted-by-NNFusion"><a href="#Executables-Emitted-by-NNFusion" class="headerlink" title="Executables Emitted by NNFusion"></a>Executables Emitted by NNFusion</h3><p>Some DL compilers generate executables statically linked with kernel libraries.</p>
<p>It is easier to decompile NNFusion- and XLA-emitted executables since they contain warpper function to invoke target operator implementations in kernel libraries. </p>
<p>运行时，通过Pin恢复网络拓扑和劫持通过warpper发送的数据。劫持的数据包括dimensions和指向参数的pointers. 然后从内存中获取参数。</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p><strong>RQ1 (Comprehensiveness and Correctness):</strong> Is BTD comprehensive and correct to process all operators used in common DL models compiled with <strong>different compilers</strong> and <strong>optimization options</strong>?</p>
<ul>
<li><p>Predicting DNN Operator Type: Glow has 14 types and TVM has 30. </p>
</li>
<li><p>数据偏差：基于统计分布的模型仍然会考虑到某一操作经常出现在另一操作之后使得分类结果偏向于出现概率大的类别，从而产生错误。</p>
</li>
<li><p>具有相似汇编代码的操作：也会存在误分类</p>
</li>
<li><p><strong>DNN Network Topology Recovery:</strong> compare the recovered network topology with the reference DNN’s computation graphs.</p>
</li>
<li><p><strong>Parameter and Dimension Recovery:</strong> Except for TVM -O0, it is difficult to compare the recovered dimensions/parameters with the reference due to compiler optimization.</p>
</li>
<li><p><strong>Recompilation</strong> They re-implement DNN models in PyTorch using recovered DNN models, then export models as ONNX files and compiled into DNN executables using the same compilation provenance.<br>compare the predicted labels and confidence scores yielded by recompiled and reference executables over every input from validation dataset.</p>
</li>
<li><p><strong>Decompiling NNFusion Outputs</strong> </p>
</li>
<li><p><strong>Other Models</strong> NLP Models, Audio Processing Models.</p>
</li>
</ul>
<p><strong>RQ2 (Robutness):</strong> Is BTD robust to survive frequent DL compiler implementation changes?</p>
<p>Evaluated BTD with prior versions of DL compilers released in the past two years.</p>
<p>BTD is robust enough against changes in current and prior versions of DL compilers. We anticipate that compiler changes are unlikely to affect the robustness of BTD in the near future.</p>
<p><strong>RQ3 (Extensibility):</strong> Can BTD be easily extended to support new operators and models? What efforts are needed?</p>
<p>Recovery of parameters/dimensions of complex operator. Supporting a new operator may need new or existing patterns. Symbolic constraints are generally human readable. We typically need several hours to desion and validate a new pattern for operators without complex optimization.</p>
<p>Users experienced in DL models can spend reasonable effort to add support for new operators and models by modifying existing patterns in BTD.</p>
<p><strong>RQ4 (Error Fixing):</strong> How does BTD handle decompilation errors?</p>
<p>On one hand, with rules presented in Sec. 6, BTD can detect and automatically fix the errors exposed when decompiling the ResNet18 executable.</p>
<p>To cope with decompilation defects, BTD provides error detection &amp; automated fixing mechanism, including a collection of rules derived from domain-specific knowledge and observations.</p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>DL compilers produce distinct executables on GPUs and CPUs. For example, TVM creates a standalone DNN executable on CPU, but a runtime library, including detailed model information, and an OpenCL/CUDA executable on GPU.</p>

	
	</div>
  <a type="button" href="/2022/10/25/Decompiling-x86-Deep-Neural-Network-Executables/#more" class="btn btn-default more">阅读此文</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-10-21 </div>
			<div class="article-title"><a href="/2022/10/21/NeurDP/" >NeurDP</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>近年来，神经网络在代码理解和逆向工程领域取得了广泛的应用。一些工作，例如著名的Github Copilot和CodeBERT等使用神经网络来学习源码的表示，在代码生成、补全和注释生成等方面取得了较好的效果。同样的，许多工作研究神经网络在二进制或汇编代码理解中的利用，从而帮助分析人员进行反汇编、恶意代码检测等。神经网络在程序分析和逆向工程中的应用大幅度帮助分析人员节省了时间成本。并且，和传统的方法相比，其速度通常较快，在有GPU的条件下可以很好的和下游任务模型进行结合，实现大批量快速的逆向工程和程序分析。</p>
<p>反编译器是逆向工程和程序分析中常用的工具，对于漏洞发现和恶意代码分析有重要作用。反编译过程可以看作是将低级程序语言翻译到和它功能等价的高级程序语言的过程。传统的反编译器的开发需要很多逆向工程师总结其程序分析经验，将其制定成规则，并结合程序分析技术来实现。著名的开源反编译器RetDec有上百个开发者曾为其贡献代码，并且开发多年，至今仍然不完善。</p>
<p>机器翻译模型在自然语言翻译的领域取得了重大进展。一些研究者受到自然语言翻译技术以及AI在代码领域中的发展的启发，开始探索使用端到端的机器翻译模型来将低级程序语言翻译成高级程序语言。这些方法尽管对于输入和输出进行了各种预处理，设计了各种模型结构，但是他们仍然无法解决优化后二进制的反编译问题。</p>
<h2 id="研究困难"><a href="#研究困难" class="headerlink" title="研究困难"></a>研究困难</h2><p>编译器优化在真实的项目中是广泛使用的。常用的GCC和Clang编译器都是默认开启O1级别的优化。编译器常常是两段式的结构，分为前端和后端，分别进行机器无关的优化和机器相关的优化。经过对编译技术的分析和实验，我们发现训练一个能处理优化后二进制代码的端到端的反编译模型非常困难。因为模型常常是数据驱动的，训练代码翻译模型需要高质量的低级语言和高级语言代码对。然而由于编译器优化策略，优化后的低级语言和高级语言在文本所包含的信息上常常难以对应，并且其结构差异很大。已有的神经网络反编译方法在解决优化问题上主要存在两个难点：</p>
<p>1）编译优化策略通常大幅度改变源码的结构。这种修改不仅体现在指令类型等层面，更多的是结构上的改变，有一些优化策略甚至会提前对一些语句进行运算，预测其结果，例如常量传播。这种改变会导致源码中有很多冗余的文本，其语义在二进制代码中本身就不存在。端到端的训练需要学习优化策略，例如上文提到的算术运算，训练这样的模型是很困难的。</p>
<p>2）对低级程序语言（二进制文件）和高级程序语言（源码文件）进行划分，并形成准确的对应关系是不容易的。以往的工作通常使用函数或者基本块进行划分，但是函数和基本块内的语句数量是没有上限的，对于较大的基本块或函数，模型很难对其进行翻译。根据调试信息划分面临着和难点1一样的问题。而如果设置最大窗口顺序划分指令序列，又会面临对应不准确的问题。</p>
<h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>NeurDP主要解决当前基于神经网络的反编译方法无法解决优化后二进制反编译的问题。其中预处理部分对LPL进行一些静态分析，包括SSA形式转换，和一些简单的寄存器传播生成低级中间语言LIR。OTU是方法的核心部分，根据数据依赖关系将LIR和HIR划分为更小且可以准确匹配的代码片段。HIR是基于LLVM IR简化版本的高级中间语言。NeurDP模型部分使用图神经网络GNN学习LIR的数据依赖图，并生成HIR指令序列。HIR Generation对生成的指令序列操作数进行识别和填充。最终，结合函数的参数、返回值以及控制结构，并使用规则将HIR转换为HPL，从而生成高级语言函数。</p>
<p><img src="NeurDP-overview.jpg" alt=""></p>
<p>为了解决困难1，本方法使用中间表示IR作为连接LPL和HPL的桥梁。IR经过了编译器前端优化，因此其文本上包含的信息和结构与LPL更加的接近。而从IR到HPL通过制定规则可以很容易实现。因此，我们选择用模型去完成从LPL到IR的翻译。</p>
<p>为了解决困难2，本方法设计了一种基于数据依赖图的切分方式，将基本块切分成更小的代码片段。基于我们的分析，绝大多数的优化策略都是基于数据流分析的。优化策略通常不会改变基本块的输入和输出的对应关系。两个不同输出在数据依赖图中的子图存在重叠部分和独立部分。考虑到对一个输出的优化不能对另一个输出的结果产生影响，优化往往是在这些重叠或独立部分的内部发生的。因此，这些子图在HIR和LIR中可以较准确地进行对应。我们按照这个原则对LIR和HIR的基本块进行划分，获得更细粒度且对应准确的LIR-HIR代码片段对作为训练集。</p>
<h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>我们测试了其他网络在使用源码，AST和IR时代码翻译的表现和NeurDP进行对比，结果如下表。可以看到使用IR之后普通的序列模型准确率也有大幅度地提升。NeurDP使用图神经网络对输入进行编码，效果比其他序列模型更好。</p>
<p><img src="NeurDP-Table6.jpg" alt=""></p>
<p>我们测试了使用不同的基本块切分方式的代码翻译的表现，结果如下图，可以看到基于数据流的划分方式比顺序滑动窗口效果明显提升。而使用我们的OTU比普通的数据流的窗口效果更好。</p>
<p><img src="NeurDP-Table5.jpg" alt=""></p>
<p>为了验证NeurDP对优化后代码的处理能力，我们将他与其他两个神经网络反编译工作Neutron和Coda以及著名的开源反编译工具RetDec在clang的O0-O3优化级别下的准确率进行了对比，结果如下表。可以看到NeurDP效果比其他工具都好，并且对于不同优化级别的代码，NeurDP的表现差异较小。RetDec由于其功能还不完善，出现了很多函数无法识别的错误。</p>
<p><img src="NeurDP-Table4.jpg" alt=""></p>
<p>Strip是一个从二进制中去掉调试和符号信息的工具，会增加反编译的难度。我们对比了在保留所有调试和符号信息、去掉调试信息和去掉所有调试和符号信息的条件下，NeurDP在O0-O3三个优化级别上的表现。结果如下表，可以看到NeurDP在去掉符号信息的情况下也表现良好。</p>
<p>我们测试使用的数据集公开在了<a target="_blank" rel="noopener" href="https://github.com/zijiancogito/neur-dp-data">https://github.com/zijiancogito/neur-dp-data</a> 。</p>

	
	</div>
  <a type="button" href="/2022/10/21/NeurDP/#more" class="btn btn-default more">阅读此文</a>
</div>

	       
		     
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2022-10-21 </div>
			<div class="article-title"><a href="/2022/10/21/DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler/" >DnD: A Cross-Architecture Deep Neural Network Decompiler</a></div>						
		</h3>
	


		     <div class="entry">
  <div class="row">
	
	
		<h1 id="DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler"><a href="#DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler" class="headerlink" title="DnD: A Cross-Architecture Deep Neural Network Decompiler"></a>DnD: A Cross-Architecture Deep Neural Network Decompiler</h1><p>Ruoyu Wu, Purdue University</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper proposes DnD, the first compiler and ISA-agnostic DNN decompiler.<br>It aims to lift the binary code compiled from a DNN on <strong>edge-device</strong> to a novel intermediate representation, able to express the high-level mathematical DNN operations.<br>They evaluate DnD on two compilers (<strong>Glow and TVM</strong>) and three ISAs (<strong>Thump, AArch64, and x86-64</strong>).<br>Dnd enables extracting the DNN models used by real-wrold micro-controllers and attacking them using white-box adversarial machine learning techniques.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>Traditional decompilers cannot capture the mathematical semantics of compiled DNN models.</p>
</li>
<li><p>DnD works with a compiled DNN model and recover its parameters, hyper-parameters and topology, and express the decompiled model in a high-level representation, encoded in the ONNX modeling language.</p>
</li>
<li><p>Techniques: 1. Uses <strong>symboloc execution</strong> in conjunction with a dedicated loop analysis to capture precise mathematical formulas representing how different DNN operators process the received data. 2. Uses a novel IR to express the high-level mathematical DNN operations in a compiler- and ISA-agnostic way. 3. Identifies the type and location of the DNN operators in a target binary by matching the extracted mathematical operations with template mathematical DNN operations, recovering hyper-parameters and parameters of all the identified DNN operators, as well as the overall network topology.</p>
</li>
<li><p>The recovered DNN model can be used to boost adversarial attacks against the original DNN, enabling the usage of the white-box attacks, in place of less efficient black-box ones.</p>
</li>
<li><p>Contributions: design and implement DnD (including decompiling of stripped binaries), design IR</p>
</li>
<li><p>Artifacts: <a target="_blank" rel="noopener" href="https://github.com/purseclab/DnD">https://github.com/purseclab/DnD</a></p>
</li>
</ul>
<h2 id="Background-and-Motivation"><a href="#Background-and-Motivation" class="headerlink" title="Background and Motivation"></a>Background and Motivation</h2><ul>
<li><p>ONNX: the open standard for ML interoperability developed by Linux Foundation.</p>
</li>
<li><p>DNN Operators: the building blocks of DNNs. A DNN operator takes the output of previous operators as its input and computes its output based on its operator type and its parameters.(174 different DNN operators defined in ONNX).</p>
</li>
<li><p>DNN Hyper-parameters and Parameters: (1) the algorithm hyper-parameters: only used during the training phase (2) the model hyper-parameters: define the netwrok structure and how the operators function (Total number of operators and the type of each operator. The DNN topology. The attributes of each operator that define its detailed semantics.)</p>
</li>
<li><p>DNN Compilers: Glow, TVM, XLA, NNFusion. Frontend-Backend component. </p>
</li>
<li><p>Frontend: transforms a DNN model into a high-level IR and performs hardware-independent optimizations, such as operator fusion.</p>
</li>
<li><p>Backend: transforms a high-level IR to a low-level IR and performs hardware-specific optimizations (vectorization and loop-related optimizations).</p>
</li>
<li><p>Compilation Scheme: interpreter-based and ahead-of-time(AOT) compilation schemes.</p>
</li>
<li><p>Interpreter-based: generate DNN binaries at runtime.They usually produce two artifacts: a DNN configuration file describing the DNN model and a runtime library that contains all the DNN operator implementations.</p>
</li>
<li><p>AOT: Specialize the operator implementation for the specific compiled operator instance’s context.</p>
</li>
<li><p>Glow and TVM: An application feeds the input data to this inference function and obtains the predicted label as output.</p>
</li>
</ul>
<h2 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h2><h4 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow."></a>Workflow.</h4><p>(1) <strong>DNN Operator Location Identification.</strong> Recovers the CFG and identifies the location of inference function and DNN operators from the input (stripped) DNN binary.</p>
<p>(2) <strong>Operator Symmary Generation.</strong> </p>
<ul>
<li>Conducts loop analysis to identify loops’ information. </li>
<li>Leverages loop’s information to perform selective symbolic execution that extracts the output of a DNN operator as symbolic expressions of its input and parameters, which capture the mathematical semantic of a DNN operator.</li>
<li>Lift the symbolic expressions to the operator symmary of a DNN operator in their IR format includes the ASTs and other information.</li>
<li>Generates template ASTs through the afore-mentioned operator summary generation.</li>
</ul>
<p>(3) DNN Model Lifting<br>Lifts each operator summary to a DNN operator and convert it to a high-level DNN representation (i.e., an ONNX model).</p>
<ul>
<li>Matches the AST in each operator summary with a template AST to determine its DNN operator type.</li>
<li>Recovers the DNN topology by identify the data dependencies between DNN operators.</li>
<li>Recovers each DNN operator’s attributes and parameters leveraging the identified DNN operator type and DNN topology, and converts the fully-recovered DNN model to an ONNX model.</li>
</ul>
<p>In summary, they first recover the functions and CFGs. Then, they identify the intresting functions (inference function and NN operators). Then, they extract the symbolic expressions by SSE and transform them to their IR. Then, they <strong>match the IR with their AST template and determines the NN operator type</strong>. Then, they <strong>recovers the topology by identifying the data dependencies between NN operators</strong>. Finally, they recovers each NN operator’s <strong>attributes and parameters</strong> using the operator type and topology and convert it to ONNX language.</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h4 id="Generality"><a href="#Generality" class="headerlink" title="Generality"></a>Generality</h4><p>How many commonly-used DNN operators and models can be suported.</p>
<p>Support 59 (84%) DNN operators.<br>Fully support 30 (81%) DNN models out of the collected 37 DNN models.</p>
<h4 id="Correctness-Accross-different-DNN-compilers-ISAs-and-DNN-models"><a href="#Correctness-Accross-different-DNN-compilers-ISAs-and-DNN-models" class="headerlink" title="Correctness Accross different DNN compilers, ISAs, and DNN models"></a>Correctness Accross different DNN compilers, ISAs, and DNN models</h4><p>Compare the model architecture (operators and topology) and inference results of original DNN models and decompiled DNN models.</p>
<p>-Models: MNIST, MobileNets v2, ResNet v1</p>
<ul>
<li><p>ISAs: Thumb, AArch64, x86-64</p>
</li>
<li><p>Decompiler: Glow, TVM</p>
</li>
</ul>
<p>Evaluated 15 DNN binaries in total.</p>
<p>没有说明测试的二进制是否是strip的</p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><ul>
<li>Extraction Attack</li>
<li>Boosting Adversarial Attacks</li>
</ul>
<h2 id="Discussion-and-Limitations"><a href="#Discussion-and-Limitations" class="headerlink" title="Discussion and Limitations"></a>Discussion and Limitations</h2><ul>
<li><p>Compilers (XLA, NNFusion) which generate DNN binaries linked with open-souce mathematical libraries to leverage the tensor operations of these libraries. To support these additional compilers, we will need to implement a dedicated analysis to identify these tensor-specific library functions. This analysis could take advantage of function matching approaches.</p>
</li>
<li><p>Decompiling Binary on DNN Acceleartors. GPUs, FPGAs have very diverse ISAs that are usually not supported by the general-purpose disassemblers and the symbolic execution framework. <strong>NVIDIA provides closed-source disassemblers cuobjdump and nvidiaasm</strong>, which translate the CUDA binary into SASS assembly code. However, most details of SASS assembly code are kept secret.</p>
</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>reverse engineering techniques targeting smart contract[1], control firmware[2] and Bluetooth firmware[3].</p>
<p>[1] Yi Zhou, Deepak Kumar, Surya Bakshi, Joshua Mason, Andrew Miller, and Michael Bailey. Erays: reverse engineering ethereum’s opaque smart contracts. In Proceedings of the USENIX Security Symposium (Usenix SEC), 2018.</p>
<p>[2] Taegyu Kim, Aolin Ding, Sriharsha Etigowni, Pengfei Sun, Jizhou Chen, Luis Garcia, Saman Zonouz, Dongyan Xu, and Dave (Jing) Tian. Reverse engineering and retroﬁtting robotic aerial vehicle control ﬁrmware using dispatch. In Proceedings of the ACM International Conference on Mobile Systems, Applications, and Services (MobiSys), 2022.</p>
<p>[3] Jianliang Wu, Ruoyu Wu, Daniele Antonioli, Mathias Payer, Nils Ole Tippenhauer, Dongyan Xu, Dave Jing Tian, and Antonio Bianchi. LIGHTBLUE: Automatic Proﬁle-Aware debloating of bluetooth stacks. In Proceedings of the USENIX Security Symposium (Usenix SEC), 2021.</p>

	
	</div>
  <a type="button" href="/2022/10/21/DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler/#more" class="btn btn-default more">阅读此文</a>
</div>

	       
	     </div>
	     <div>
	       <center>
	         <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

	       </center>
	     </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="搜索" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			<div class="widget">
    
	    <h4 class="dsq-widget-title">最新留言</h4>
		<div id="recent-comments"></div>
		<script type="text/javascript">
		    getRecentCommentsList({
			   type: "github" ? "github" : "github",
			   user: "zijiancogito",
               repo: "comment",
               client_id: "b79cbe842579a612ed08",
               client_secret: "6585bb42b7c5b2fedf088cfce6b7e811f8803fe3",
			   count: "5" ? "5" : 5,
			   recent_comments_target: "#recent-comments"
			});
		</script>
	
</div>

		
			
	<div class="widget">
		<h4>分类</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/骈文/最喜欢/">最喜欢<span>1</span></a></li>
		
			<li><a href="/categories/编译/">编译<span>3</span></a></li>
		
			<li><a href="/categories/骈文/">骈文<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>标签云</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/Decompiler/">Decompiler<span>3</span></a></li>
		
			<li><a href="/tags/NN-Decompiler/">NN Decompiler<span>2</span></a></li>
		
			<li><a href="/tags/置顶/">置顶<span>1</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2022/10/27/Symbolic-Execution-Tech/" ><i class="fa fa-file-o"></i>Symbolic Execution Tech</a>
      </li>
    
      <li>
        <a href="/2022/10/25/Decompiling-x86-Deep-Neural-Network-Executables/" ><i class="fa fa-file-o"></i>Decompiling x86 Deep Neural...</a>
      </li>
    
      <li>
        <a href="/2022/10/21/NeurDP/" ><i class="fa fa-file-o"></i>NeurDP</a>
      </li>
    
      <li>
        <a href="/2022/10/21/DnD-A-Cross-Architecture-Deep-Neural-Network-Decompiler/" ><i class="fa fa-file-o"></i>DnD: A Cross-Architecture D...</a>
      </li>
    
      <li>
        <a href="/2022/10/20/滕王阁序/" ><i class="fa fa-file-o"></i>滕王阁序</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>链接</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/zijiancogito" title="My Github account." target="_blank"]);">My Github</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2022 Zijian
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->

  <script>
  marked.setOptions({
    highlight: function (code, lang) {
        return hljs.highlightAuto(code).value;
    }
  });
  function Highlighting(){
    var markdowns = document.getElementsByClassName('markdown');
    for(var i=0;i<markdowns.length;i++){
        if(markdowns[i].innerHTML) markdowns[i].innerHTML =marked(markdowns[i].innerHTML);
    }
  }
  window.addEventListener('DOMContentLoaded', Highlighting, false);
  window.addEventListener('load', Highlighting, false);
  </script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


</body>
</html>